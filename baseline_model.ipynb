{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from read_files import X,y\n",
    "from read_files import USERCODE_X\n",
    "from utils import tokenize_tweet\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting all text for all authors \n",
    "\n",
    "def collect(array):\n",
    "\n",
    "    train_author = [None for i in range(array.shape[0])]\n",
    "\n",
    "    for i, author in enumerate(array):\n",
    "        buf = StringIO()\n",
    "        for tweet in author:\n",
    "            buf.write(tweet + \"\")\n",
    "        single_text = buf.getvalue()\n",
    "        train_author[i] = single_text\n",
    "\n",
    "    train_author = np.array(train_author)\n",
    "    return train_author \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting bigrams \n",
    "tknzr = TweetTokenizer()\n",
    "vectorizer = CountVectorizer(analyzer='word', tokenizer = tknzr.tokenize, ngram_range=(2, 2))\n",
    "X_train_f = vectorizer.fit_transform(collect(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_f = vectorizer.transform(collect(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train_f, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7380952380952381"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word accuracy\n",
    "accuracy_score(y_test, clf.predict(X_test_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(5, 5))\n",
    "X_train_f = vectorizer.fit_transform(collect(X_train))\n",
    "X_test_f = vectorizer.transform(collect(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8412698412698413"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#char accuracy\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train_f, y_train)\n",
    "accuracy_score(y_test, clf.predict(X_test_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8415119925417407"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, clf.predict(X_test_f), average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.83333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(n_components=10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(X_train_f))\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train_f.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45295345 0.20643717 0.13691326 0.03973111 0.01404132 0.01236087\n",
      " 0.01044976 0.00860926 0.00760826 0.00658638]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(probability=  True)\n",
    "clf.fit(pca.transform(X_train_f.toarray()), y_train)\n",
    "#accuracy_score(y_test, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37484311, 0.62515689],\n",
       "       [0.52446532, 0.47553468],\n",
       "       [0.99428864, 0.00571136],\n",
       "       [0.18044511, 0.81955489],\n",
       "       [0.0819303 , 0.9180697 ],\n",
       "       [0.78794548, 0.21205452],\n",
       "       [0.24590154, 0.75409846],\n",
       "       [0.87969818, 0.12030182],\n",
       "       [0.97850421, 0.02149579],\n",
       "       [0.98968573, 0.01031427],\n",
       "       [0.2256982 , 0.7743018 ],\n",
       "       [0.79020658, 0.20979342],\n",
       "       [0.79499108, 0.20500892],\n",
       "       [0.98983224, 0.01016776],\n",
       "       [0.97839503, 0.02160497],\n",
       "       [0.55845308, 0.44154692],\n",
       "       [0.93513248, 0.06486752],\n",
       "       [0.04535589, 0.95464411],\n",
       "       [0.16374774, 0.83625226],\n",
       "       [0.15087133, 0.84912867],\n",
       "       [0.68349526, 0.31650474],\n",
       "       [0.98515475, 0.01484525],\n",
       "       [0.25709595, 0.74290405],\n",
       "       [0.82896203, 0.17103797],\n",
       "       [0.81576148, 0.18423852],\n",
       "       [0.66740548, 0.33259452],\n",
       "       [0.88586802, 0.11413198],\n",
       "       [0.0485514 , 0.9514486 ],\n",
       "       [0.24939075, 0.75060925],\n",
       "       [0.20023012, 0.79976988],\n",
       "       [0.08624384, 0.91375616],\n",
       "       [0.6118396 , 0.3881604 ],\n",
       "       [0.88548052, 0.11451948],\n",
       "       [0.08026346, 0.91973654],\n",
       "       [0.25630984, 0.74369016],\n",
       "       [0.08735327, 0.91264673],\n",
       "       [0.15078318, 0.84921682],\n",
       "       [0.9970318 , 0.0029682 ],\n",
       "       [0.07176638, 0.92823362],\n",
       "       [0.98615354, 0.01384646],\n",
       "       [0.63061793, 0.36938207],\n",
       "       [0.0591394 , 0.9408606 ],\n",
       "       [0.14997897, 0.85002103],\n",
       "       [0.62040641, 0.37959359],\n",
       "       [0.05957675, 0.94042325],\n",
       "       [0.75779122, 0.24220878],\n",
       "       [0.17947173, 0.82052827],\n",
       "       [0.06345846, 0.93654154],\n",
       "       [0.07748117, 0.92251883],\n",
       "       [0.09918735, 0.90081265],\n",
       "       [0.98825028, 0.01174972],\n",
       "       [0.91714294, 0.08285706],\n",
       "       [0.74329741, 0.25670259],\n",
       "       [0.79824768, 0.20175232],\n",
       "       [0.42184789, 0.57815211],\n",
       "       [0.95242125, 0.04757875],\n",
       "       [0.99340628, 0.00659372],\n",
       "       [0.7939005 , 0.2060995 ],\n",
       "       [0.09547355, 0.90452645],\n",
       "       [0.21054337, 0.78945663],\n",
       "       [0.97816766, 0.02183234],\n",
       "       [0.97857823, 0.02142177],\n",
       "       [0.92897158, 0.07102842],\n",
       "       [0.97323856, 0.02676144],\n",
       "       [0.52087135, 0.47912865],\n",
       "       [0.99108405, 0.00891595],\n",
       "       [0.0934957 , 0.9065043 ],\n",
       "       [0.98436082, 0.01563918],\n",
       "       [0.10443426, 0.89556574],\n",
       "       [0.97168246, 0.02831754],\n",
       "       [0.35398857, 0.64601143],\n",
       "       [0.41105703, 0.58894297],\n",
       "       [0.97328832, 0.02671168],\n",
       "       [0.96472168, 0.03527832],\n",
       "       [0.5794301 , 0.4205699 ],\n",
       "       [0.11966381, 0.88033619],\n",
       "       [0.40745747, 0.59254253],\n",
       "       [0.85105064, 0.14894936],\n",
       "       [0.75329683, 0.24670317],\n",
       "       [0.9302555 , 0.0697445 ],\n",
       "       [0.07443853, 0.92556147],\n",
       "       [0.28757509, 0.71242491],\n",
       "       [0.08277118, 0.91722882],\n",
       "       [0.76715524, 0.23284476],\n",
       "       [0.06010813, 0.93989187],\n",
       "       [0.09171146, 0.90828854],\n",
       "       [0.09037774, 0.90962226],\n",
       "       [0.58552772, 0.41447228],\n",
       "       [0.09141657, 0.90858343],\n",
       "       [0.28732633, 0.71267367],\n",
       "       [0.48801848, 0.51198152],\n",
       "       [0.05590284, 0.94409716],\n",
       "       [0.66734233, 0.33265767],\n",
       "       [0.06878054, 0.93121946],\n",
       "       [0.97227846, 0.02772154],\n",
       "       [0.95438706, 0.04561294],\n",
       "       [0.9932207 , 0.0067793 ],\n",
       "       [0.98919428, 0.01080572],\n",
       "       [0.33834665, 0.66165335],\n",
       "       [0.09816852, 0.90183148],\n",
       "       [0.07455146, 0.92544854],\n",
       "       [0.03540326, 0.96459674],\n",
       "       [0.33294809, 0.66705191],\n",
       "       [0.06015846, 0.93984154],\n",
       "       [0.55017013, 0.44982987]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(pca.transform(X_test_f.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASELINE BIGRAM MODEL \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
    "X_train_f = vectorizer.fit_transform(collect(X_train))\n",
    "X_test_f = vectorizer.transform(collect(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(probability = True)\n",
    "clf.fit(X_train_f, y_train)\n",
    "baseline_pred_proba = clf.predict_proba(X_test_f)\n",
    "baseline_pred_proba = clf.predict_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(probability = False)\n",
    "clf.fit(X_train_f, y_train)\n",
    "baseline_pred = clf.predict(X_test_f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Train Features\n",
      "Processing features: pos_counts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:57<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Predict Processing for features: pos_counts, is complete!\n",
      "Processing features: author_style_counts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:14<00:00, 21.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Predict Processing for features: author_style_counts, is complete!\n",
      "Processing features: lix_score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 203/315 [00:04<00:02, 49.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:06<00:00, 48.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Predict Processing for features: lix_score, is complete!\n",
      "Processing features: get_sent_polarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:16<00:00, 18.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Predict Processing for features: get_sent_polarity, is complete!\n",
      "Processing features: seperated_punctuation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:00<00:00, 417.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Predict Processing for features: seperated_punctuation, is complete!\n",
      "Processing features: tf_idf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:06<00:00, 49.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji TF_IDF Processing for features: tf_idf, is complete!\n",
      "Processing features: tf_idf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:06<00:00, 45.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profanity TF_IDF Processing for features: tf_idf, is complete!\n",
      "Processing features: tf_idf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:17<00:00, 17.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words TF_IDF Processing for features: tf_idf, is complete!\n",
      "Generating Test Features\n",
      "Processing features: pos_counts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:19<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Predict Processing for features: pos_counts, is complete!\n",
      "Processing features: author_style_counts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:05<00:00, 20.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Predict Processing for features: author_style_counts, is complete!\n",
      "Processing features: lix_score\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:02<00:00, 47.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Predict Processing for features: lix_score, is complete!\n",
      "Processing features: get_sent_polarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:05<00:00, 18.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Predict Processing for features: get_sent_polarity, is complete!\n",
      "Processing features: seperated_punctuation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:00<00:00, 447.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Predict Processing for features: seperated_punctuation, is complete!\n",
      "Processing features: tf_idf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:02<00:00, 47.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Predict Processing for features: tf_idf, is complete!\n",
      "Processing features: tf_idf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:02<00:00, 44.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Predict Processing for features: tf_idf, is complete!\n",
      "Processing features: tf_idf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:05<00:00, 17.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words TF_IDF Processing for features: tf_idf, is complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## OUR MODEL\n",
    "\n",
    "from classifier_methods import *\n",
    "\n",
    "X_train_F, emoji_pca, profanity_pca, word_pca, emoji_tfidf, profanity_tfidf, words_tfidf = get_features_train(X_train)\n",
    "X_test_F = get_features_test(X_test, emoji_pca, profanity_pca, word_pca, emoji_tfidf, profanity_tfidf, words_tfidf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train_F, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_proba = classifier.predict_proba(X_test_F)\n",
    "model_pred = classifier.predict(X_test_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('proba_results.txt', 'w', encoding = 'utf-8')\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    file.write(f\"{baseline_pred_proba[i][0]}, {baseline_pred_proba[i][1]},  {model_pred_proba[i][0]}, {model_pred_proba[i][1]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "y = y_test\n",
    "X = np.loadtxt('proba_results.txt', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(random_state=1, max_iter=1000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load XML files complete, number of tweet profiles:  420\n",
      "Tokenizing and vectorizing input...\n",
      "Initializing K-fold splits...\n",
      "Split 0 Accuracy: 0.8333333333333334\n",
      "Split 0 Accuracy: 0.8809523809523809\n",
      "Split 0 Accuracy: 0.8690476190476191\n",
      "Split 0 Accuracy: 0.7976190476190477\n",
      "Split 0 Accuracy: 0.8333333333333334\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('<U3'), dtype('<U3')) -> dtype('<U3')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/yananikolova/Documents/GitHub/zenodo-PAN22-aut-prof-irony-stereotype/baseline_model.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yananikolova/Documents/GitHub/zenodo-PAN22-aut-prof-irony-stereotype/baseline_model.ipynb#ch0000033?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbaseline\u001b[39;00m \u001b[39mimport\u001b[39;00m baseline_svm\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yananikolova/Documents/GitHub/zenodo-PAN22-aut-prof-irony-stereotype/baseline_model.ipynb#ch0000033?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mread_files\u001b[39;00m \u001b[39mimport\u001b[39;00m X,y\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yananikolova/Documents/GitHub/zenodo-PAN22-aut-prof-irony-stereotype/baseline_model.ipynb#ch0000033?line=3'>4</a>\u001b[0m baseline_svm(X,y)\n",
      "File \u001b[0;32m~/Documents/GitHub/zenodo-PAN22-aut-prof-irony-stereotype/baseline.py:58\u001b[0m, in \u001b[0;36mbaseline_svm\u001b[0;34m(data, labels, tokenize, kfold)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/yananikolova/Documents/GitHub/zenodo-PAN22-aut-prof-irony-stereotype/baseline.py?line=54'>55</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSplit \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m Accuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy_score(y_test, prediction)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/yananikolova/Documents/GitHub/zenodo-PAN22-aut-prof-irony-stereotype/baseline.py?line=55'>56</a>\u001b[0m     accuracies\u001b[39m.\u001b[39mappend(prediction)\n\u001b[0;32m---> <a href='file:///Users/yananikolova/Documents/GitHub/zenodo-PAN22-aut-prof-irony-stereotype/baseline.py?line=57'>58</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAverage accuracy:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39msum\u001b[39;49m(accuracies)\u001b[39m/\u001b[39mkfold)\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U3'), dtype('<U3')) -> dtype('<U3')"
     ]
    }
   ],
   "source": [
    "from baseline import baseline_svm\n",
    "from read_files import X,y\n",
    "\n",
    "baseline_svm(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from read_files import X,y\n",
    "from read_files import USERCODE_X\n",
    "from utils import tokenize_tweet\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def collect(array):\n",
    "\n",
    "    train_author = [None for i in range(array.shape[0])]\n",
    "\n",
    "    for i, author in enumerate(array):\n",
    "        buf = StringIO()\n",
    "        for tweet in author:\n",
    "            buf.write(tweet + \"\")\n",
    "        single_text = buf.getvalue()\n",
    "        train_author[i] = single_text\n",
    "\n",
    "    train_author = np.array(train_author)\n",
    "    return train_author \n",
    "\n",
    "def baseline_svm(data, labels, tokenize = 'char', kfold = 5):\n",
    "\n",
    "    print(\"Tokenizing and vectorizing input...\")\n",
    "\n",
    "    if tokenize == 'word':\n",
    "        tknzr = TweetTokenizer()\n",
    "        vectorizer = CountVectorizer(analyzer='word', tokenizer = tknzr.tokenize, ngram_range=(2, 2))\n",
    "        X = vectorizer.fit_transform(collect(data))\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
    "        X = vectorizer.fit_transform(collect(data))\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    print(\"Initializing K-fold splits...\")\n",
    "\n",
    "    i= 0\n",
    "    kf = KFold(n_splits=5)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        clf = svm.SVC()\n",
    "        clf.fit(X_train, y_train)\n",
    "        prediction = clf.predict(X_test)\n",
    "        print(f\"Split {i} Accuracy: {accuracy_score(y_test, prediction)}\")\n",
    "        accuracies.append(accuracy_score(y_test, prediction))\n",
    "        i+= 1\n",
    "\n",
    "    print(\"Average accuracy:\", sum(accuracies)/kfold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and vectorizing input...\n",
      "Initializing K-fold splits...\n",
      "Split 0 Accuracy: 0.8333333333333334\n",
      "Split 1 Accuracy: 0.8809523809523809\n",
      "Split 2 Accuracy: 0.8690476190476191\n",
      "Split 3 Accuracy: 0.7976190476190477\n",
      "Split 4 Accuracy: 0.8333333333333334\n",
      "<class 'list'> <class 'numpy.float64'>\n",
      "Average accuracy: 0.8428571428571429\n"
     ]
    }
   ],
   "source": [
    "baseline_svm(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae83aaff4c47202ead0cb5b0cfe74444d437ac818c9c2cd6826845dc75a11708"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
